{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('NLP': venv)",
   "display_name": "Python 3.8.5 64-bit ('NLP': venv)",
   "metadata": {
    "interpreter": {
     "hash": "2136a9c3637fd160483224d7922e48bf03b650be5dff26724a0c1f8d1279953b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This script contain a variety of additional feature relevant when using scikit-learn. This script is mean as an example and should provide inspiration for finding the best classifier.\n",
    "\n",
    "---\n",
    "## List of Classifiers \n",
    "There is a lot of classifiers in scikit-learn for a list you could examine this thread [here](https://stackoverflow.com/questions/41844311/list-of-all-classification-algorithms) on stackoverflow, it does not contain all classifiers though. Classifiers from external packages are such as XGboost is not included. For future proofing I have also added the list in the end of the document.\n",
    "\n",
    "---\n",
    "## Pipelines\n",
    "scikit-learn allows you to make pipelines which can end up becoming very convenient when testing out multiple classifiers. For instance:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Our model obtained a performance accuracy of 0.84\n"
    }
   ],
   "source": [
    "from classification import read_imdb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "imdb = read_imdb()\n",
    "X_train, X_test, y_train, y_test = train_test_split(imdb.text, imdb.tag)\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "acc = sum(predictions == y_test)/len(y_test)\n",
    "print(f\"Our model obtained a performance accuracy of {acc}\")\n"
   ]
  },
  {
   "source": [
    "---\n",
    "## Cross-Validation\n",
    "If you compare the performance of the pipeline classification and the classification in the script `classification.py` you are likely to see that the two performances aren't the same and can very quite a bit. To solve this we can do cross-validation. Cognitive Science bachelor student should be familiar with cross-validation, but if you aren't please see [this video](https://www.youtube.com/watch?v=fSytzGwwBVw)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.785  0.83   0.7975 0.8375 0.8225]\nmean: 0.8145 \n SD: 0.019962464777677123\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# perform a 5-fold cross validation\n",
    "scores = cross_val_score(text_clf, X=imdb.text, y=imdb.tag, cv=5, scoring='accuracy')\n",
    "print(scores)\n",
    "import numpy as np\n",
    "print(\"mean:\", np.mean(scores), \"\\n SD:\", np.std(scores))"
   ]
  },
  {
   "source": [
    "---\n",
    "## Grid Search\n",
    "\n",
    "A lot of the ML methods need you specify some hyperparameters. If you have only a weak idea of how to set these you can use a grid search to search through the parameters. This is simply a method which goes through all possible combination of the specified hyperparameters.\n",
    "\n",
    "**Note:** Remember the computation time increase quite a lot. To calculate it multiple the length of each list of hyperparameters you are searching over together. E.g. for the example below we have 3 parameter for C and 2 for kernel and thus have a $2\\cdot3=6$ times increase in computation (this does not include the computational cost of the specific hyperparameters).\n",
    "\n",
    "You can naturally also create a loop and loop through all the different classifiers. It will probably take quite a while though ;) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The cross validates score was:\n0.8335)\n\nThe best parameters where:\n {'svm__C': 0.1, 'svm__kernel': 'linear'}\n"
    }
   ],
   "source": [
    "from sklearn.svm import SVC  # this is another type of classifer called a support vector machine\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# hyperparameters to search over\n",
    "param_grid = {'svm__kernel':('linear', 'rbf'),  # change the kernel\n",
    "              'svm__C': [0.1, 1, 10]}           # change the C hyperparameter\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('svm', SVC())])\n",
    "\n",
    "\n",
    "search = GridSearchCV(text_clf, param_grid, \n",
    "                      cv=5,       # 5 fold cross-validation for each\n",
    "                      n_jobs=-1)  # indicate that you want to run on all the cores of the computer \n",
    "search.fit(X=imdb.text, y=imdb.tag)\n",
    "print(f\"The cross validates score was:\\n{round(search.best_score_, 4)})\")\n",
    "print(\"\\nThe best parameters where:\\n\", search.best_params_)"
   ]
  },
  {
   "source": [
    "## The not quite full list of classifiers for Scikit-learn\n",
    "\n",
    "Missing candidates include (maybe more):\n",
    "* Xgboost\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm.classes import OneClassSVM\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifierCV\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \n",
    "from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.mixture import GMM \n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import VBGMM\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}